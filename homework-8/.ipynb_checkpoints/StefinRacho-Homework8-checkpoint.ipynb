{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 8\n",
    "### Due Friday, 12/6/2024\n",
    "### SPOT Evaluations\n",
    "In class, we used some of Dr. Morsony's SPOT data to look at different versions of a t-test and ANOVA.  We also used them to look at what standard deviation actually means for discrete (rather than continuous) data.\n",
    "\n",
    "We saw that, at least for one class, the answers to the questions are statistically the same.  And the differences got even smaller if we corrected for the data being discreet.\n",
    "\n",
    "In this assignment, I'd like to see if we can go further and try to see is there really any information in these SPOT surveys?\n",
    "\n",
    "In the SPOT_surveys directory are .csv files for 5 classes.  We're going to take a look at all of them.\n",
    "\n",
    "(There is also a bunch more classes with PDF files only.  10 bonus points (1 free homework) to anyone who can figure out how to extract the data from these files.  At minimum, I'd like to get the n, av., and dev. for each question, but ideally I'd like the number of 1s, 2s, 3s, for each question as well from the histograms.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) First, let's just see if the question responses are the same or different for each class.  Load each class (probably with a loop) and use the different ANOVA tests (ANOVA, Alexander Govern, and Kurskal) for all 9 questions in each class.  Do any of the classes have significant results (p<0.05)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats as scipy_stats\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2024-ASTR3000-001.csv\n",
      "ANOVA Result: F_onewayResult(statistic=np.float64(0.8630019405881474), pvalue=np.float64(0.5485311902636981))\n",
      "Kruskal-Wallis Result: KruskalResult(statistic=np.float64(7.4683616164839055), pvalue=np.float64(0.4870423405035633))\n",
      "Alexander Govern Result: AlexanderGovernResult(statistic=np.float64(8.41263598960705), pvalue=np.float64(0.39423455225713155))\n",
      " \n",
      "ANOVA: No significant result (p-value = 0.5485311902636981)\n",
      "Kruskal-Wallis: No significant result (p-value = 0.4870423405035633)\n",
      "Alexander Govern: No significant result (p-value = 0.39423455225713155)\n",
      "------------------------------------------------------------\n",
      " \n",
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2024-ASTR2100-001.csv\n",
      "ANOVA Result: F_onewayResult(statistic=np.float64(1.3594591512412386), pvalue=np.float64(0.21663296127570275))\n",
      "Kruskal-Wallis Result: KruskalResult(statistic=np.float64(11.197374569243005), pvalue=np.float64(0.19076453308815805))\n",
      "Alexander Govern Result: AlexanderGovernResult(statistic=np.float64(9.59542146828085), pvalue=np.float64(0.2945773258920515))\n",
      " \n",
      "ANOVA: No significant result (p-value = 0.21663296127570275)\n",
      "Kruskal-Wallis: No significant result (p-value = 0.19076453308815805)\n",
      "Alexander Govern: No significant result (p-value = 0.2945773258920515)\n",
      "------------------------------------------------------------\n",
      " \n",
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2024-PHYS4530-001.csv\n",
      "ANOVA Result: F_onewayResult(statistic=np.float64(0.4326923076923076), pvalue=np.float64(0.8738933857683934))\n",
      "Kruskal-Wallis Result: KruskalResult(statistic=np.float64(3.4207317073170675), pvalue=np.float64(0.9052538324367548))\n",
      "Alexander Govern Result: AlexanderGovernResult(statistic=nan, pvalue=nan)\n",
      " \n",
      "ANOVA: No significant result (p-value = 0.8738933857683934)\n",
      "Kruskal-Wallis: No significant result (p-value = 0.9052538324367548)\n",
      "Alexander Govern: No significant result (p-value = nan)\n",
      "------------------------------------------------------------\n",
      " \n",
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2023-ASTR2100--1.csv\n",
      "ANOVA Result: F_onewayResult(statistic=np.float64(0.9590964171731978), pvalue=np.float64(0.46923092736432226))\n",
      "Kruskal-Wallis Result: KruskalResult(statistic=np.float64(5.542789205159851), pvalue=np.float64(0.698294837940574))\n",
      "Alexander Govern Result: AlexanderGovernResult(statistic=np.float64(5.910057333552225), pvalue=np.float64(0.6573062070793438))\n",
      " \n",
      "ANOVA: No significant result (p-value = 0.46923092736432226)\n",
      "Kruskal-Wallis: No significant result (p-value = 0.698294837940574)\n",
      "Alexander Govern: No significant result (p-value = 0.6573062070793438)\n",
      "------------------------------------------------------------\n",
      " \n",
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2023-PHYS4400-01.csv\n",
      "ANOVA Result: F_onewayResult(statistic=np.float64(0.7710554371002133), pvalue=np.float64(0.6304524604954398))\n",
      "Kruskal-Wallis Result: KruskalResult(statistic=np.float64(6.551052329313181), pvalue=np.float64(0.5857506242958863))\n",
      "Alexander Govern Result: AlexanderGovernResult(statistic=np.float64(5.3969332417673535), pvalue=np.float64(0.7144302073080129))\n",
      " \n",
      "ANOVA: No significant result (p-value = 0.6304524604954398)\n",
      "Kruskal-Wallis: No significant result (p-value = 0.5857506242958863)\n",
      "Alexander Govern: No significant result (p-value = 0.7144302073080129)\n",
      "------------------------------------------------------------\n",
      " \n",
      "No significant results found in any class.\n"
     ]
    }
   ],
   "source": [
    "survey_directory = \"/home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/\"\n",
    "\n",
    "csv_files = glob.glob(survey_directory + '*.csv')\n",
    "csv_files = np.asarray(csv_files)\n",
    "num_questions = 9\n",
    "significant_results = []\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f'File: {file}')\n",
    "    data = np.genfromtxt(\n",
    "        file,\n",
    "        skip_header=1,\n",
    "        usecols=np.arange(2, 2 + num_questions),\n",
    "        delimiter=',',\n",
    "        encoding='windows-1252',\n",
    "        filling_values=np.nan\n",
    "    )\n",
    "    \n",
    "    questions_data = []\n",
    "    for q in range(num_questions):\n",
    "        score = data[:, q]\n",
    "        valid_scores = score[(score >= 1) & (~np.isnan(score))]\n",
    "        questions_data.append(valid_scores)\n",
    "    \n",
    "    anova_result = scipy_stats.f_oneway(*questions_data)\n",
    "    print(f'ANOVA Result: {anova_result}')\n",
    "    \n",
    "    kruskal_result = scipy_stats.kruskal(*questions_data)\n",
    "    print(f'Kruskal-Wallis Result: {kruskal_result}')\n",
    "    \n",
    "    alexander_result = scipy_stats.alexandergovern(*questions_data)\n",
    "    print(f'Alexander Govern Result: {alexander_result}')\n",
    "    print(' ')\n",
    "    \n",
    "    # Check for significant results (p-value < 0.05)\n",
    "    if anova_result.pvalue < 0.05:\n",
    "        print(f'ANOVA: Significant result detected (p-value = {anova_result.pvalue})')\n",
    "        significant_results.append((file, 'ANOVA', anova_result.pvalue))\n",
    "    else:\n",
    "        print(f'ANOVA: No significant result (p-value = {anova_result.pvalue})')\n",
    "    \n",
    "    if kruskal_result.pvalue < 0.05:\n",
    "        print(f'Kruskal-Wallis: Significant result detected (p-value = {kruskal_result.pvalue})')\n",
    "        significant_results.append((file, 'Kruskal-Wallis', kruskal_result.pvalue))\n",
    "    else:\n",
    "        print(f'Kruskal-Wallis: No significant result (p-value = {kruskal_result.pvalue})')\n",
    "\n",
    "    if alexander_result.pvalue < 0.05:\n",
    "        print(f'Alexander Govern: Significant result detected (p-value = {alexander_result.pvalue})')\n",
    "        significant_results.append((file, 'Alexander Govern', alexander_result.pvalue))\n",
    "    else:\n",
    "        print(f'Alexander Govern: No significant result (p-value = {alexander_result.pvalue})')\n",
    "    \n",
    "    print('------------------------------------------------------------')\n",
    "    print(' ')\n",
    "\n",
    "if significant_results:\n",
    "    for result in significant_results:\n",
    "        print(f'File: {result[0]}, Test: {result[1]}, p-value: {result[2]}')\n",
    "else:\n",
    "    print('No significant results found in any class.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) In class we saw an example of including the discrete error correction for Welch's t-test by using random trials for many test and averaging the resulting p-values.  Extend this to the ANOVA testing (ANOVA, Alexander Govern, and Kurskal) trials.  Then run the responses for each class through.  How do the new (average) p-values compare to the p-values from part a)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2024-ASTR3000-001.csv\n",
      "ANOVA: Original p-value = 0.5485, Average p-value after correction = 0.5324\n",
      "ANOVA: Not significant after correction.\n",
      "Kruskal-Wallis: Original p-value = 0.4870, Average p-value after correction = 0.4999\n",
      "Kruskal-Wallis: Not significant after correction.\n",
      "Alexander Govern: Original p-value = 0.3942, Average p-value after correction = 0.4643\n",
      "Alexander Govern: Not significant after correction.\n",
      "------------------------------------------------------------\n",
      " \n",
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2024-ASTR2100-001.csv\n",
      "ANOVA: Original p-value = 0.2166, Average p-value after correction = 0.3399\n",
      "ANOVA: Not significant after correction.\n",
      "Kruskal-Wallis: Original p-value = 0.1908, Average p-value after correction = 0.3259\n",
      "Kruskal-Wallis: Not significant after correction.\n",
      "Alexander Govern: Original p-value = 0.2946, Average p-value after correction = 0.3806\n",
      "Alexander Govern: Not significant after correction.\n",
      "------------------------------------------------------------\n",
      " \n",
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2024-PHYS4530-001.csv\n",
      "ANOVA: Original p-value = 0.8739, Average p-value after correction = 0.7888\n",
      "ANOVA: Not significant after correction.\n",
      "Kruskal-Wallis: Original p-value = 0.9053, Average p-value after correction = 0.7161\n",
      "Kruskal-Wallis: Not significant after correction.\n",
      "Alexander Govern: Original p-value = nan, Average p-value after correction = 0.6923\n",
      "Alexander Govern: Not significant after correction.\n",
      "------------------------------------------------------------\n",
      " \n",
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2023-ASTR2100--1.csv\n",
      "ANOVA: Original p-value = 0.4692, Average p-value after correction = 0.4964\n",
      "ANOVA: Not significant after correction.\n",
      "Kruskal-Wallis: Original p-value = 0.6983, Average p-value after correction = 0.5645\n",
      "Kruskal-Wallis: Not significant after correction.\n",
      "Alexander Govern: Original p-value = 0.6573, Average p-value after correction = 0.5321\n",
      "Alexander Govern: Not significant after correction.\n",
      "------------------------------------------------------------\n",
      " \n",
      "File: /home/stefin/Documents/csu-stan/cs4010/homework-8/SPOT_surveys/Spring_2023-PHYS4400-01.csv\n",
      "ANOVA: Original p-value = 0.6305, Average p-value after correction = 0.5726\n",
      "ANOVA: Not significant after correction.\n",
      "Kruskal-Wallis: Original p-value = 0.5858, Average p-value after correction = 0.5552\n",
      "Kruskal-Wallis: Not significant after correction.\n",
      "Alexander Govern: Original p-value = 0.7144, Average p-value after correction = 0.5981\n",
      "Alexander Govern: Not significant after correction.\n",
      "------------------------------------------------------------\n",
      " \n",
      "No significant results found in any class after error correction.\n"
     ]
    }
   ],
   "source": [
    "significant_results = []\n",
    "num_trials = 1000\n",
    "measurement_error_std = 0.5\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f'File: {file}')\n",
    "    \n",
    "    data = np.genfromtxt(\n",
    "        file,\n",
    "        skip_header=1,\n",
    "        usecols=np.arange(2, 2 + num_questions),\n",
    "        delimiter=',',\n",
    "        encoding='windows-1252',\n",
    "        filling_values=np.nan\n",
    "    )\n",
    "    \n",
    "    questions_data = []\n",
    "    for q in range(num_questions):\n",
    "        score = data[:, q]\n",
    "        valid_scores = score[(score >= 1) & (~np.isnan(score))]\n",
    "        questions_data.append(valid_scores)\n",
    "    \n",
    "    anova_result = scipy_stats.f_oneway(*questions_data)\n",
    "    kruskal_result = scipy_stats.kruskal(*questions_data)\n",
    "    alexander_result = scipy_stats.alexandergovern(*questions_data)\n",
    "    \n",
    "    p_values = {\n",
    "        'ANOVA': [],\n",
    "        'Kruskal-Wallis': [],\n",
    "        'Alexander Govern': []\n",
    "    }\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        simulated_data = []\n",
    "        for q in range(num_questions):\n",
    "            noise = np.random.normal(loc=0.0, scale=measurement_error_std, size=questions_data[q].shape)\n",
    "            simulated_scores = questions_data[q] + noise\n",
    "            simulated_data.append(simulated_scores)\n",
    "        \n",
    "        sim_anova = scipy_stats.f_oneway(*simulated_data)\n",
    "        p_values['ANOVA'].append(sim_anova.pvalue)\n",
    "        \n",
    "        sim_kruskal = scipy_stats.kruskal(*simulated_data)\n",
    "        p_values['Kruskal-Wallis'].append(sim_kruskal.pvalue)\n",
    "        \n",
    "        sim_alexander = scipy_stats.alexandergovern(*simulated_data)\n",
    "        p_values['Alexander Govern'].append(sim_alexander.pvalue)\n",
    "    \n",
    "    average_p_values = {}\n",
    "    for test in p_values:\n",
    "        valid_pvals = [p for p in p_values[test] if not np.isnan(p)]\n",
    "        average_p = np.mean(valid_pvals)\n",
    "        average_p_values[test] = average_p\n",
    "    \n",
    "    tests = [\n",
    "        ('ANOVA', anova_result.pvalue),\n",
    "        ('Kruskal-Wallis', kruskal_result.pvalue),\n",
    "        ('Alexander Govern', alexander_result.pvalue)\n",
    "    ]\n",
    "    \n",
    "    for test_name, original_p in tests:\n",
    "        avg_p = average_p_values.get(test_name)\n",
    "        print(f'{test_name}: Original p-value = {original_p:.4f}, Average p-value after correction = {avg_p:.4f}')\n",
    "        if avg_p < 0.05:\n",
    "            print(f'{test_name}: Significant after correction.')\n",
    "            significant_results.append((file, test_name, avg_p))\n",
    "        else:\n",
    "            print(f'{test_name}: Not significant after correction.')\n",
    "    \n",
    "    print('------------------------------------------------------------')\n",
    "    print(' ')\n",
    "\n",
    "if significant_results:\n",
    "    for result in significant_results:\n",
    "        print(f'File: {result[0]}, Test: {result[1]}, Average p-value: {result[2]:.4f}')\n",
    "else:\n",
    "    print('No significant results found in any class after error correction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We'd also like to know if there are differences between classes.  There's three ways I cna think to do this.  One would be to take all the responses to all the questions for each class and do and ANOVA for the 5 classes.  But the problem is the respones in each class aren't independant, so lumping them together isn't really valid.  (So don't do this.)\n",
    "\n",
    "A second was is to treat the respones to each question as a data set (so you have 5*9=45 data sets) and do a big ANOVA (etc.) on all of them.  Give this a try, using the methods in part a) and b).  What results to you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) The third way would be to treat each question seperately and compare the classes for that question with an ANOA.  Give this a try.  What p-values do you get for each question?  And there any questions where (at least) one class gave a significantly different response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Really what were doing isn't totally valid anyway - the reponses to the SPOT survey aren't actually numbers, they're strongly agree, agree, etc.  Really, a 1 or 2 is a \"postive\" reponse and an 4 or 5 is a \"negative\" response, and a 3 is \"neutral\".  If we group \"postive\" and \"negative\" repsones, and irgnore neutral respones (which is not a great idea), we can treat the repsones as a binomial distribution.  Doing this, we can estimate how many respones we need in a class to get useful data.  For example, how many students would need to respond for you to be confident more students gave positive respones than negative responses?  You can calculate this using the Chernoff_trials, if you pick a good value for p, delta, and epsilon.  What might some good values be?  How many respones are needed for those values?\n",
    "\n",
    "(A one-sided version of the Chernoff_tails calculation might actually be more useful here, but don't worry about coming up with one for the homework.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Based on your results, do you think the survey questions in SPOT contain much (statistically valid) information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
