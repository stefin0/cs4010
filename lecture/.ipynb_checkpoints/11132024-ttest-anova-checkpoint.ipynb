{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc6c9a2-d304-43dd-b7e4-43006a0349b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import scipy.stats as scipy_stats\n",
    "\n",
    "import scipy.linalg as scipy_linalg\n",
    "\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "import scipy.special as special"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d55c8-e102-4abe-9580-fd46e9f50ac0",
   "metadata": {},
   "source": [
    "## T-test\n",
    "\n",
    "<h3>\n",
    "\n",
    "We are going to continue with hypothesis testing.  Here, we are going to have two (or more) groups of numbers and we want to see if they are different.\n",
    "\n",
    "What does this mean?\n",
    "\n",
    "We really want to see if the numbers are drawn from different populations.  We saw an example of this for flipping coins - we asked is the number of heads consistent with a fair coin.\n",
    "\n",
    "Here, instead of comparing one group of numbers (number of heads) to a set probability, we are going to compare two groups of numbers to each other.  These numbers aren't going to be just 0 and 1.  They can (and preferably should) be over a larger range or can be continuois.\n",
    "\n",
    "\n",
    "One example of this would be students taking exams.  Say I give a pre-test before and a post-test after going over some topic.  From the scores on the tests, I want to know if the students' knowledge increase (or at least if they did better on the post-test).\n",
    "\n",
    "The answer will depend on the distribution of the scores on the two test and the differences between them.\n",
    "\n",
    "For each group of scores, we can calculate a mean and a standard deviation.  Then we can calcualte a \"t-stastitic\" based on the difference in mean divided by the join standard deviation.  \n",
    "\n",
    "This is called a t-test.  (Formally, this version is a Student's t-test.  This assumes equal sample sizes and known, identical, and unbiased standard deviations.)\n",
    "\n",
    "\n",
    "Let's do a really dumb example first:  \n",
    "\n",
    "I flip a coin 20 times and get 7 heads and I flip another coin 20 times and get 13 heads.  Are the coins the same?\n",
    "\n",
    "To answer this, we can use a t-test as follows:\n",
    "\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a44603aa-152a-49f9-a2da-3fa54c19d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35 0.48936048492959283\n",
      "0.65 0.4893604849295929\n",
      "Difference of mean =  -0.30000000000000004\n",
      "t-statistic =  -1.9386185179765925\n"
     ]
    }
   ],
   "source": [
    "# Calculate the t-statistic for equal sample sizes\n",
    "\n",
    "# This is going to be an approximation, because we can only get heads or tails, not all values in between.\n",
    "\n",
    "# First, make our data:\n",
    "\n",
    "data_1 = np.zeros(20)\n",
    "data_1[0:7] = 1\n",
    "\n",
    "data_2 = np.zeros(20)\n",
    "data_2[0:13] = 1\n",
    "\n",
    "n_1 = data_1.size\n",
    "n_2 = data_2.size\n",
    "\n",
    "mean_1 = np.mean(data_1)\n",
    "mean_2 = np.mean(data_2)\n",
    "\n",
    "#sd_1 = np.std(data_1)   # This is the population standard deviation.   \n",
    "#sd_2 = np.std(data_2)\n",
    "\n",
    "sd_1 = np.std(data_1,ddof=1)  # For a sample standard devation.\n",
    "sd_2 = np.std(data_2,ddof=1)\n",
    "\n",
    "print(mean_1,sd_1)\n",
    "print(mean_2,sd_2)\n",
    "\n",
    "diff_of_mean = mean_1-mean_2\n",
    "\n",
    "pooled_standard_deviation = np.sqrt((sd_1**2 + sd_2**2)/2)\n",
    "\n",
    "t_statistic = diff_of_mean/(pooled_standard_deviation*np.sqrt(2/n_1))\n",
    "\n",
    "print('Difference of mean = ',diff_of_mean)\n",
    "\n",
    "print('t-statistic = ',t_statistic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a46498-6abb-474f-a414-17d10c76fcd7",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "So we get a t-statistic of 1.94.  I don't know what that means.  But what we can do is use this to find the p-value, the likelihood that the samples are drawn from different distributions.  \n",
    "\n",
    "It's easier to use built-in funcntions for the t-test from scipy.stats, which does this for us.\n",
    "\n",
    "We can also use this to calcualte the 95% confidence interval (or another value if we want to) for the difference in the mean of the two populations.\n",
    "    \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e8f2e3e-560a-4845-a22f-3905bf9578f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-1.9386185179765925, pvalue=0.06000178954876174, df=38.0)\n",
      " \n",
      "Difference of mean =  -0.30000000000000004\n",
      "p-value =  0.06000178954876174\n",
      " \n",
      "Difference in mean ConfidenceInterval(low=-0.6132737274208395, high=0.013273727420839376)\n"
     ]
    }
   ],
   "source": [
    "# Let use some built-in methods to do the t-test:\n",
    "\n",
    "students_ttest = scipy_stats.ttest_ind(data_1,data_2,equal_var=True)\n",
    "\n",
    "print(students_ttest)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('Difference of mean = ',diff_of_mean)\n",
    "\n",
    "print('p-value = ',students_ttest.pvalue)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('Difference in mean',students_ttest.confidence_interval(confidence_level=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88897abd-b902-4f1d-bfe3-1a0218f3c344",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "This tells us that the samples are consistent with the coin being the same - the p-value is >0.05 and the 95% confidence internval between the means overlaps zero.\n",
    "\n",
    "If our sample sizes and/or standard deviations are not the same, we should use Welch's t-test instead.  For this example, the results will be the same:\n",
    "    \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c754174-bb51-4f72-adbe-d1ef4cc7fe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TtestResult(statistic=-1.9386185179765925, pvalue=0.06000178954876174, df=38.0)\n",
      " \n",
      "Difference of mean =  -0.30000000000000004\n",
      "p-value =  0.06000178954876174\n",
      " \n",
      "Difference in mean ConfidenceInterval(low=-0.6132737274208395, high=0.013273727420839376)\n"
     ]
    }
   ],
   "source": [
    "# Welch's t-test\n",
    "\n",
    "welch_ttest = scipy_stats.ttest_ind(data_1,data_2,equal_var=False)\n",
    "\n",
    "print(welch_ttest)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('Difference of mean = ',diff_of_mean)\n",
    "\n",
    "print('p-value = ',welch_ttest.pvalue)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('Difference in mean',welch_ttest.confidence_interval(confidence_level=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681399c-40f0-439b-921a-69d5f4b6ebab",
   "metadata": {},
   "source": [
    "#\n",
    "# SPOT Data\n",
    "#\n",
    "\n",
    "<h3>\n",
    "\n",
    "Let's look at some real data.  Each semester, I do SPOT surveys of students in my classes.  There are 9 questions, and students answer 1 to 5 for each, with 1 being \"strongly agree\", 2 being \"agree\", 3 being \"neither agree not disagree\", 4 being \"disagree\", and 5 being \"strongly disagree\".\n",
    "\n",
    "Using these values, we can get a mean adn standard deviation for each question.\n",
    "\n",
    "There's a lot we can do with this data.  But a first thing to do is just see if the responses between two questions are the same or different.\n",
    "\n",
    "Let's start by loading some data.  The responses are in a csv file (at least for some SPOT results), os we can load them and look at the responses for a couple of questions:\n",
    "\n",
    "    \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0e9cff6-33ae-4b1f-a841-95b89dcb9d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/Spring_2023-ASTR2100--1.csv'\n",
      " '/Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/Spring_2023-PHYS4400-01.csv'\n",
      " '/Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/Spring_2024-ASTR2100-001.csv'\n",
      " '/Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/Spring_2024-ASTR3000-001.csv'\n",
      " '/Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/Spring_2024-PHYS4530-001.csv']\n",
      "Spring_2024-ASTR2100-001.csv\n",
      "(23, 11)\n",
      "[1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 3. 2. 1. 1. 1.]\n",
      "[1. 3. 2. 1. 2. 1. 1. 1. 1. 2. 2. 3. 2. 3. 2. 3. 2. 1. 2. 3. 3. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Test loading .csv files\n",
    "\n",
    "# ~/Documents/csustan/RPT/SPOT_evaluations/Spring_2024-ASTR2100-001.csv\n",
    "#data = np.loadtxt('/Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/Spring_2024-ASTR2100-001.csv',skiprows=1,usecols=11,encoding='windows-1252')\n",
    "\n",
    "\n",
    "csv_files = ! ls /Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/*.csv\n",
    "csv_files = np.asarray(csv_files)\n",
    "print(csv_files)\n",
    "\n",
    "file = csv_files[0]\n",
    "\n",
    "file = 'Spring_2024-ASTR2100-001.csv'\n",
    "\n",
    "print(file)\n",
    "\n",
    "\n",
    "#data = np.genfromtxt('/Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/Spring_2024-ASTR2100-001.csv',skip_header=1,usecols=np.arange(11),delimiter=',',encoding='windows-1252',filling_values=np.nan)\n",
    "#data = np.genfromtxt('/Users/brianmorsony/Documents/csustan/RPT/SPOT_evaluations/Spring_2024-ASTR3000-001.csv',skip_header=1,usecols=np.arange(11),delimiter=',',encoding='windows-1252',filling_values=np.nan)\n",
    "\n",
    "data = np.genfromtxt(file,skip_header=1,usecols=np.arange(11),delimiter=',',encoding='windows-1252',filling_values=np.nan)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# The responses start at column 2\n",
    "\n",
    "score1 = data[:,2]\n",
    "score1 = score1[np.argwhere(score1>=1)].flatten()\n",
    "\n",
    "# Use question 3 as our second data set.  Question 1 and 2 have the same mean.\n",
    "\n",
    "score2 = data[:,4]\n",
    "score2 = score2[np.argwhere(score2>=1)].flatten()\n",
    "\n",
    "print(score1)\n",
    "print(score2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9da64-2e6f-44ca-a9ab-37901bd5026e",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "So we loaded some data and have the responses to two questions.  \n",
    "\n",
    "First, let's just look at the mean and standard deviation.  Are these responses different from 1.0 (all strongly agree)?\n",
    "\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4d49ce0-5240-4773-975e-f4723021c037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score1 mean = 1.565217391304348\n",
      "score2 mean = 1.9130434782608696\n",
      "score1 std dev = 0.5897678246195884\n",
      "score2 std dev = 0.7927537436201203\n",
      "score1 std dev of the mean = 0.12297509238026913\n",
      "score2 std dev of the mean = 0.16530058234250186\n"
     ]
    }
   ],
   "source": [
    "mean_1 = np.mean(score1)\n",
    "mean_2 = np.mean(score2)\n",
    "\n",
    "sd_1 = np.std(score1,ddof=1)  # For a sample standard devation.\n",
    "sd_2 = np.std(score2,ddof=1)\n",
    "\n",
    "print('score1 mean =',mean_1)\n",
    "print('score2 mean =',mean_2)\n",
    "\n",
    "print('score1 std dev =',sd_1)\n",
    "print('score2 std dev =',sd_2)\n",
    "\n",
    "sd_of_mean_1 = sd_1/np.sqrt(score1.size)\n",
    "sd_of_mean_2 = sd_2/np.sqrt(score2.size)\n",
    "\n",
    "print('score1 std dev of the mean =',sd_of_mean_1)\n",
    "print('score2 std dev of the mean =',sd_of_mean_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ed546-0da9-4cf0-8326-5a185b08dd24",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "Both sets of responses are within 2 standard deviations of 1, but the standard deviation of the mean, which is smaller by $\\sqrt{n}$, is not consistent with 1.\n",
    "\n",
    "However, this is probably really underestimating the variance.  We'll come back to this later.\n",
    "\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39365e82-4e68-439b-b609-8a651c80ca5d",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "Are these respones to thse two questions the same or different?\n",
    "\n",
    "Let's use Welch's t-test to answer this (some questions have a different number of respones, and the variance is not equal).\n",
    "    \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29fd06a2-2130-4b21-87e6-b5e4d2b3004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score1 mean = 1.565217391304348\n",
      "score2 mean = 1.9130434782608696\n",
      " \n",
      "TtestResult(statistic=-1.6882542548886996, pvalue=0.09902275120975886, df=40.64187736563209)\n",
      " \n",
      "Difference of mean =  -0.34782608695652173\n",
      "p-value =  0.09902275120975886\n",
      " \n",
      "Difference in mean ConfidenceInterval(low=-0.764017580331009, high=0.06836540641796551)\n"
     ]
    }
   ],
   "source": [
    "# Welch's t-test\n",
    "\n",
    "mean_1 = np.mean(score1)\n",
    "mean_2 = np.mean(score2)\n",
    "\n",
    "print('score1 mean =',mean_1)\n",
    "print('score2 mean =',mean_2)\n",
    "\n",
    "\n",
    "diff_of_mean = mean_1-mean_2\n",
    "\n",
    "print(' ')\n",
    "\n",
    "welch_ttest = scipy_stats.ttest_ind(score1,score2,equal_var=False)\n",
    "\n",
    "print(welch_ttest)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('Difference of mean = ',diff_of_mean)\n",
    "\n",
    "print('p-value = ',welch_ttest.pvalue)\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print('Difference in mean',welch_ttest.confidence_interval(confidence_level=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfaf03c-af0d-46ba-b2a2-4b84edefaa3b",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "So for these two quesitons, there a 10% change they are drawn from the same distribution and no differnce (0 difference in mean) is well within the 95% confidence limit - the resposnese are statstically the same.\n",
    "\n",
    "But we have more than 2 questions.  Let look at the others:\n",
    "    \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c071d011-5a34-4271-9611-2ed37afc1fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Difference of mean 1 and 2 =  0.0\n",
      "Difference in mean ConfidenceInterval(low=-0.3504987085989154, high=0.3504987085989154)\n",
      "p-value =  1.0\n",
      " \n",
      "Difference of mean 1 and 3 =  -0.34782608695652173\n",
      "Difference in mean ConfidenceInterval(low=-0.764017580331009, high=0.06836540641796551)\n",
      "p-value =  0.09902275120975886\n",
      " \n",
      "Difference of mean 1 and 4 =  0.21739130434782616\n",
      "Difference in mean ConfidenceInterval(low=-0.1927266651235826, high=0.6275092738192349)\n",
      "p-value =  0.29067243354871997\n",
      " \n",
      "Difference of mean 1 and 5 =  -0.08695652173913038\n",
      "Difference in mean ConfidenceInterval(low=-0.4765377743948589, high=0.30262473091659814)\n",
      "p-value =  0.6547878302004508\n",
      " \n",
      "Difference of mean 1 and 6 =  -0.025691699604742935\n",
      "Difference in mean ConfidenceInterval(low=-0.4048545466982949, high=0.35347114748880903)\n",
      "p-value =  0.8918776544542344\n",
      " \n",
      "Difference of mean 1 and 7 =  0.18426501035196696\n",
      "Difference in mean ConfidenceInterval(low=-0.14686973276562842, high=0.5153997534695623)\n",
      "p-value =  0.26777480752783595\n",
      " \n",
      "Difference of mean 1 and 8 =  0.01976284584980248\n",
      "Difference in mean ConfidenceInterval(low=-0.3610088457319326, high=0.40053453743153755)\n",
      "p-value =  0.9170667102326009\n",
      " \n",
      "Difference of mean 1 and 9 =  -0.21739130434782594\n",
      "Difference in mean ConfidenceInterval(low=-0.7245571606659417, high=0.2897745519702898)\n",
      "p-value =  0.3900575641216708\n"
     ]
    }
   ],
   "source": [
    "# Compare question 1 to all other questions:\n",
    "\n",
    "for i in np.arange(2,10):\n",
    "    #print(i)\n",
    "    score2 = data[:,1+i]\n",
    "    score2 = score2[np.argwhere(score2>=1)].flatten()\n",
    "\n",
    "    mean_2 = np.mean(score2)\n",
    "    diff_of_mean = mean_1-mean_2\n",
    "    welch_ttest = scipy_stats.ttest_ind(score1,score2,equal_var=False)\n",
    "\n",
    "    print(' ')\n",
    "    \n",
    "    print('Difference of mean 1 and',i,'= ',diff_of_mean)\n",
    "    print('Difference in mean',welch_ttest.confidence_interval(confidence_level=0.95))    \n",
    "    print('p-value = ',welch_ttest.pvalue)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ebe51-648b-42fe-8386-b799ee4d6b88",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "All our p-values are more that 0.05, so all are consistent with the same distributiuon as question 1.  But even if they weren't, we'd exepct 1/20 to be less that 0.05, so we really need a smallet p-value for 8 questions.  Maybe 0.005? or 0.05/8 = 0.00625?\n",
    "\n",
    "And, we should really test every pair of question responses.  \n",
    "\n",
    "We can make a loop to do this, but it might not be the best approach:\n",
    "\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bebad6ec-9a80-4de1-ba70-5f0a1f6d6e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39005756 0.39005756 0.63546074 0.11623389 0.62336608 0.46494771\n",
      "  0.10808889 0.36791841 1.        ]\n",
      " [0.39005756 0.39005756 0.63546074 0.11623389 0.62336608 0.46494771\n",
      "  0.10808889 0.36791841 1.        ]\n",
      " [0.39005756 0.39005756 0.63546074 0.11623389 0.62336608 0.46494771\n",
      "  0.10808889 0.36791841 1.        ]\n",
      " [0.39005756 0.39005756 0.63546074 0.11623389 0.62336608 0.46494771\n",
      "  0.10808889 0.36791841 1.        ]\n",
      " [0.39005756 0.39005756 0.63546074 0.11623389 0.62336608 0.46494771\n",
      "  0.10808889 0.36791841 1.        ]\n",
      " [0.33019825 0.33019825 0.73544284 0.097071   0.54145795 0.39790531\n",
      "  0.08962698 0.31241111 0.9098901 ]\n",
      " [0.27460486 0.27460486 0.84542675 0.07980115 0.46194154 0.33448875\n",
      "  0.07325788 0.26065018 0.81572936]\n",
      " [0.33019825 0.33019825 0.73544284 0.097071   0.54145795 0.39790531\n",
      "  0.08962698 0.31241111 0.9098901 ]\n",
      " [0.39005756 0.39005756 0.63546074 0.11623389 0.62336608 0.46494771\n",
      "  0.10808889 0.36791841 1.        ]]\n",
      "minimum p_value = 0.0732578841513129\n"
     ]
    }
   ],
   "source": [
    "# Make an array of p-values:\n",
    "\n",
    "p_array = np.zeros([9,9])\n",
    "\n",
    "for i in np.arange(0,9):\n",
    "    score1 = data[:,2+i]\n",
    "    score1 = score2[np.argwhere(score1>=1)].flatten()\n",
    "    \n",
    "    for j in np.arange(0,9):\n",
    "        score2 = data[:,2+j]\n",
    "        score2 = score2[np.argwhere(score2>=1)].flatten()\n",
    "    \n",
    "        mean_2 = np.mean(score2)\n",
    "        diff_of_mean = mean_1-mean_2\n",
    "        welch_ttest = scipy_stats.ttest_ind(score1,score2,equal_var=False)\n",
    "        p_array[i,j] = welch_ttest.pvalue\n",
    "\n",
    "print(p_array)\n",
    "\n",
    "print('minimum p_value =',np.min(p_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849327d-174d-4363-b660-53bfef456888",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "So for these responses, the minimum p-value is 0.073, which is still consistent with all the resposnes being the same.  The number of pairs of questions is $8+7+6+...+1 = 36$, so we'd really be looking for a p-value < 0.00139, which we don't have.\n",
    "    \n",
    "</h3>\n",
    "\n",
    "# ANOVA\n",
    "\n",
    "<h3>\n",
    "\n",
    "A simpler way to see if any of the questions have independent answers is to test all the data at once, rather than pair-by-pair.  This is call \"Analysis of Variance\" or ANOVA.  \n",
    "\n",
    "There are a few different versions of this.  ANOVA assumes the samples are independent, the distributions of measurements (responses) are normal, and all the questions have the same standard deviation.  This is probably not really true.\n",
    "\n",
    "If the variance between groups of responses is different, you should use the Alexander Govern test.\n",
    "\n",
    "If distributions are not Gaussian, you can use the Kruskal-Wallis H-test.\n",
    "\n",
    "Let's try the all and look at the resutlts:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dca153a2-33f1-4315-95fe-5aa809e61ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anova results = F_onewayResult(statistic=1.3594591512412386, pvalue=0.21663296127570325)\n",
      "Alexander Govern results = AlexanderGovernResult(statistic=9.59542146828085, pvalue=0.2945773258920515)\n",
      "Kruskal = KruskalResult(statistic=11.197374569243005, pvalue=0.19076453308815808)\n"
     ]
    }
   ],
   "source": [
    "# Try anova from scipy.stats:\n",
    "\n",
    "\n",
    "anova_result = scipy_stats.f_oneway(data[:,2],data[:,3],data[:,4],data[:,5],data[:,6],data[:,7],data[:,8],data[:,9],data[:,10],nan_policy='omit')\n",
    "\n",
    "print('Anova results =',anova_result)\n",
    "\n",
    "# How about Alexanger-Govern or Kruskal-Wallis:\n",
    "\n",
    "AG_result = scipy_stats.alexandergovern(data[:,2],data[:,3],data[:,4],data[:,5],data[:,6],data[:,7],data[:,8],data[:,9],data[:,10],nan_policy='omit')\n",
    "\n",
    "print('Alexander Govern results =',AG_result)\n",
    "\n",
    "kruskal_result = scipy_stats.kruskal(data[:,2],data[:,3],data[:,4],data[:,5],data[:,6],data[:,7],data[:,8],data[:,9],data[:,10],nan_policy='omit')\n",
    "\n",
    "#kruskal_result = stats.kruskal(*(data[:,2:10]),nan_policy='omit')\n",
    "\n",
    "print('Kruskal =',kruskal_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cdef4-8c9c-4306-a6e1-79e2c215d40a",
   "metadata": {},
   "source": [
    "<h3>\n",
    "\n",
    "For any of these test, they are all consistant with being drawn from the same distribution (p > 0.05), as we would expect.  \n",
    "\n",
    "I'm not really sure which is the proper one to use.  Probably the Alexander Govern test or whichever one gives the highest p-value.\n",
    "    \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb59bc-1279-46f9-af9d-faacd2ea56f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
